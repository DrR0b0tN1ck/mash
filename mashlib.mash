[[[
"""A library of "standard" functions that provide many of the basic operations
that one might want to do in a mash document.  Usually include this near the
start."""

# vim: set ft=python :
# pylint: disable=invalid-name
# pylint: disable=global-statement
# pylint: disable=undefined-variable
# pylint: disable=too-many-branches
# pylint: disable=no-else-return
# pylint: disable=no-else-continue


import concurrent.futures
import hashlib
import os
import resource
import shutil
import subprocess
import sys
import threading


from exceptiongroup import ExceptionGroup


# First, some initialization.  This should only happen once.
try:
    mashlib_initialized
except NameError:
    mashlib_initialized = False

if not mashlib_initialized:
    mashlib_initialized = True

    # A list of executable names whose existence we have already verified.
    executables_checked = set()

    # A limit on the number of parallel jobs to run at a time.
    max_jobs = os.cpu_count() + 4

    # The manager for the thread pool that handles expensive jobs, mostly from
    # shell().  Created on the first call that needs it, rather than here, to
    # give the client code time to change max_jobs.
    job_executor = None

    # A list of futures for jobs that have been submitted.
    job_futures = []

    # A dictionary mapping resource names to threading.Event objects.
    # Created atthe start of a job, set at job completion.
    job_resource_events = {}

    # This is the directory where started.
    original_directory = os.getcwd()
    sys.path.append(original_directory)

    # This is the directory where we will do all of our work, execute
    # commands, etc.  This will be the current directory most of the time.
    build_directory = os.path.join(original_directory, ".mash")
    sys.path.append(build_directory)

    # This directory has things built in previous runs.  If nothing has
    # changed, then we can just copy things over from there instead of
    # re-building them.
    archive_directory = os.path.join(original_directory, ".mash-archive")

    # This is the directory that should contain the completed output files.
    keep_directory = original_directory

    # This is a list of places we should search when importing a new file.
    import_search_directories = [original_directory, ]

    # Move anything from the existing build directory to the archive directory.
    if os.path.exists(build_directory):
        for file_name in os.listdir(build_directory):
            if not os.path.exists(archive_directory):
                os.makedirs(archive_directory)
            full_file_name = os.path.join(build_directory, file_name)
            full_file_name_archive = os.path.join(archive_directory, file_name)
            if os.path.isdir(full_file_name_archive):
                shutil.rmtree(full_file_name_archive)
            shutil.move(full_file_name, os.path.join(archive_directory, file_name))

    # Make sure we have a build directory and that it's the current/working
    # directory.
    if not os.path.exists(build_directory):
        os.makedirs(build_directory)
    os.chdir(build_directory)

def check_for_executable(exe):
    """Check whether some executable with the given name exists in the path.
    If it does not exist, complain."""
    if exe not in executables_checked and shutil.which(exe) is None:
        raise ValueError(f'Executable {exe} not found in path.')
    executables_checked.add(exe)

def shell(command, stdin=None, check=True, provides=None):
    """Execute the given command asyncronously in a shell.  Raise
    subprocess.CalledProcessError if the command gives a non-zero return code.
    Return a future, which when completed results in CompletedProcess object,
    which has stdout and stderr attributes.  In addition, this object has
    attributes tacked on to it showing the user time and system time consumed,
    as reported by getrusage."""

    global job_executor

    if job_executor is None:
        job_executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_jobs)

    if check:
        check_for_executable(command.split(' ')[0])

    print("  ", command)

    if provides is None:
        provides = []
    if isinstance(provides, str):
        provides = [ provides ]

    events = { res:threading.Event() for res in provides }
    job_resource_events.update(events)

    def run_job():
        usage_before = resource.getrusage(resource.RUSAGE_CHILDREN)

        completed_process = subprocess.run(command,
                                           shell=True,
                                           check=True,
                                           input=stdin,
                                           stdout=subprocess.PIPE,
                                           stderr=subprocess.PIPE,
                                           timeout=60000)
        usage_after = resource.getrusage(resource.RUSAGE_CHILDREN)
        completed_process.user_time = usage_after.ru_utime - usage_before.ru_utime
        completed_process.sys_time = usage_after.ru_stime - usage_before.ru_stime

        for _, event in events.items():
            event.set()

        return completed_process

    future = job_executor.submit(run_job)
    job_futures.append(future)
    return future

def wait_for(resources):
    """Wait for shell process creating the named resource to complete."""
    if isinstance(resources, str):
        resources = [ resources ]
    for res in resources:
        event = job_resource_events[res]
        if not event.is_set():
            print(f'[waiting for {res}]')
        event.wait()

def save(target, contents=None):
    """Ensure that the given text (default: current contents of this frame)
    exist in the build directory as a file with the given name.  Use an old
    copy from the archive directory if possible."""

    # Use the current frame text by default.
    if contents is None:
        contents = self.content

    # Are we dealing with binary data or text?
    binary = hasattr(contents, 'decode')
    if binary:
        bin_mode_flag = 'b'
    else:
        bin_mode_flag = ''

    # Check for an identically-named, identical-contents file in the archive
    # directory.
    archive_name = os.path.join(archive_directory, target)
    try:
        with open(archive_name, 'r' + bin_mode_flag) as input_file:
            existing_contents = input_file.read()
        exists_already = (existing_contents == contents)
    except IOError:
        exists_already = False

    if exists_already:
        # We have this exact file in the archive directory.  Copy it instead
        # of saving directly.  This keeps the timestamp intact, which can
        # help us elsewhere to tell if things need to be rebuilt.
        try:
            shutil.copy2(archive_name, target)
        except shutil.SameFileError:
            pass

    else:
        # We don't have a file like this anywhere.  Actually save it.
        with open(target, 'w'+bin_mode_flag) as output_file:
            output_file.write(contents)

def recall(target, *sources):
    """If any of the sources are being built by an active shell job, wait
    until that shell job finishes.  Then check whether the given target file
    exists in the archive directory, and if so, if it's newer than all of the
    given sources.  If so, copy it over and return the target.  If not, do
    nothing and return None."""

    archive_target = os.path.join(archive_directory, target)

    ok = True

    for source in sources:
        if source in job_resource_events:
            wait_for(source)

    if not os.path.exists(archive_target):
        print(f"({target}): [does not exist]", end='')
        ok = False
        target_time = None
    else:
        print(f"{target}:", end='')
        target_time = os.path.getmtime(archive_target)
        if sources:
            sources = list(dict.fromkeys(sources))
            for source in sources:
                try:
                    source_time = os.path.getmtime(source)
                except FileNotFoundError as fnfe:
                    raise FileNotFoundError(f'While checking dependencies for '
                                    f'{target}, could not find file {source}. '
                                    f'Full dependency list is {sources}.') from fnfe
                if target_time is not None and source_time > target_time:
                    print(f" ({source})", end='')
                    ok = False
                else:
                    print(f" {source}", end='')
        else:
            print(" [no sources]")

    print()

    if ok:
        if os.path.isfile(archive_target):
            shutil.copy2(archive_target, build_directory)
        else:
            if os.path.exists(target):
                shutil.rmtree(target)
            shutil.copytree(archive_target, os.path.join(build_directory, target))
        return None
    else:
        return target

def keep(file_to_keep):
    """Copy a file or directory from the working directory to
    the keep directory."""
    if not os.path.isabs(keep_directory):
        raise ValueError(f'Keep directory ({keep_directory}) is a relative path. '
                        f'This would be interpreted relative to the build directory ',
                        f'({build_directory}).  This almost certainly not what you want. '
                        f'Use an absolute path for keep_directory instead.')

    if not os.path.exists(keep_directory):
        os.makedirs(keep_directory)
    if os.path.isfile(file_to_keep):
        shutil.copy2(file_to_keep, keep_directory)
    elif os.path.isdir(file_to_keep):
        target = os.path.join(keep_directory, file_to_keep)
        if os.path.exists(target):
            shutil.rmtree(target)
        shutil.copytree(file_to_keep, target)
    elif not os.path.exists(file_to_keep):
        raise FileNotFoundError(f"Cannot keep {file_to_keep} because it does not exist.")
    else:
        raise NotImplementedError(f"Don't know how to keep {file_to_keep}, "
                                   "which is neither file nor directory.")

def read(fname):
    """Read the given file, appending to the content of the current frame."""
    with open(fname, 'r', encoding='utf-8') as it:
        self.content += it.read()

def imprt(*fnames, target=None, conditional=False):
    """Copy one or more files to the build directory, looking in the
    directories listed in import_search_directories to find them.  If target is
    given, the file (of which there must be only one) will be renamed as it is
    imported.  If conditional is True, don't complain if the files don't
    exist."""

    # Sanity check.
    if target and len(fnames) > 1:
        raise ValueError("Called imprt() with both multiple inputs and a target.")

    # Import each thing, one at a time.
    for fname in fnames:
        # Figure out the source filename.
        fr = None
        for directory in import_search_directories:
            x = os.path.join(directory, fname)
            if os.path.exists(x):
                fr = x
                break

        # If we didn't find it, maybe complain and definitely give up on this
        # one.
        if fr is None:
            if conditional:
                continue
            else:
                raise FileNotFoundError(f"Trying to import {fname} but could not "
                                         "find it in any of these places:\n"
                                         + '\n'.join(import_search_directories))

        # Figure out the destination filename.
        if not target:
            dest = os.path.basename(fname)
        else:
            dest = target
        to = os.path.join(build_directory, dest)

        # Does this file exist already?  If not, copy it in.
        if not os.path.isfile(to) or not filecmp.cmp(fr, to, False):
            print(fr)
            shutil.copy2(fr, to)

    # Return the name of the (last) imported file.
    if len(fnames) > 0:
        return os.path.basename(fnames[-1])
    else:
        return None

def anonymous_name(content=None):
    """Return an anonymous name constructed by hashing the given content or,
    by default, the contents of the current frame."""
    if content is None:
        content = self.content
    return hashlib.sha1(content.encode('utf-8')).hexdigest()[:7]

def at_end():
    """Executed automatically at the end of the run.  Finishes outstanding jobs
    and raises and exceptions that we've been saving up."""
    # Finish up any jobs that are still running.
    exceptions = []
    for future in job_futures:
        try:
            future.result()
        except Exception as exception:  #pylint: disable=broad-except
            exceptions.append(exception)

    # Report any exceptions that occured.
    if len(exceptions) == 1:
        raise exceptions[0]
    if len(exceptions) > 1:
        raise ExceptionGroup("multiple failed jobs", exceptions)

]]]
