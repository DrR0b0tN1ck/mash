[[[
"""A library of "standard" functions that provide many of the basic operations
that one might want to do in a mash document.  Usually include this near the
start."""

# vim: set ft=python :
# pylint: disable=invalid-name
# pylint: disable=global-statement
# pylint: disable=undefined-variable
# pylint: disable=too-many-branches
# pylint: disable=no-else-return


import concurrent.futures
import os
import resource
import shutil
import subprocess
import sys
import threading


from exceptiongroup import ExceptionGroup


# First, some initialization.  This should only happen once.
try:
    mashlib_initialized
except NameError:
    mashlib_initialized = False

if not mashlib_initialized:
    mashlib_initialized = True

    # A list of executable names whose existence we have already verified.
    executables_checked = set()

    # A limit on the number of parallel jobs to run at a time.
    max_jobs = os.cpu_count() + 4

    # The manager for the thread pool that handles expensive jobs, mostly from
    # shell().  Created on the first call that needs it, rather than here, to
    # give the client code time to change max_jobs.
    job_executor = None

    # A list of futures for jobs that have been submitted.
    job_futures = []

    # A dictionary mapping resource names to threading.Event objects.
    # Created atthe start of a job, set at job completion.
    job_resource_events = {}

    # This is the directory where started.
    original_directory = os.getcwd()
    sys.path.append(original_directory)

    # This is the directory where we will do all of our work, execute
    # commands, etc.  This will be the current directory most of the time.
    build_directory = os.path.join(original_directory, ".mash")
    sys.path.append(build_directory)

    # This directory has things built in previous runs.  If nothing has
    # changed, then we can just copy things over from there instead of
    # re-building them.
    archive_directory = os.path.join(original_directory, ".mash-archive")

    # This is the directory that should contain the completed output files.
    keep_directory = original_directory

    # This is a list of places we should search when importing a new file.
    import_search_directories = [original_directory, ]

    # Move anything from the existing build directory to the archive directory.
    if os.path.exists(build_directory):
        for file_name in os.listdir(build_directory):
            if not os.path.exists(archive_directory):
                os.makedirs(archive_directory)
            full_file_name = os.path.join(build_directory, file_name)
            full_file_name_archive = os.path.join(archive_directory, file_name)
            if os.path.isdir(full_file_name_archive):
                shutil.rmtree(full_file_name_archive)
            shutil.move(full_file_name, os.path.join(archive_directory, file_name))

    # Make sure we have a build directory and that it's the current/working
    # directory.
    if not os.path.exists(build_directory):
        os.makedirs(build_directory)
    os.chdir(build_directory)

def check_for_executable(exe):
    """Check whether some executable with the given name exists in the path.
    If it does not exist, complain."""
    if exe not in executables_checked and shutil.which(exe) is None:
        raise ValueError(f'Executable {exe} not found in path.')
    executables_checked.add(exe)

def shell(command, stdin=None, check=True, provides=None):
    """Execute the given command asyncronously in a shell.  Raise
    subprocess.CalledProcessError if the command gives a non-zero return code.
    Return a future, which when completed results in CompletedProcess object,
    which has stdout and stderr attributes.  In addition, this object has
    attributes tacked on to it showing the user time and system time consumed,
    as reported by getrusage."""

    global job_executor

    if job_executor is None:
        job_executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_jobs)

    if check:
        check_for_executable(command.split(' ')[0])

    print("  ", command)

    if provides is None:
        provides = []
    if isinstance(provides, str):
        provides = [ provides ]

    events = { res:threading.Event() for res in provides }
    job_resource_events.update(events)

    def run_job():
        usage_before = resource.getrusage(resource.RUSAGE_CHILDREN)

        completed_process = subprocess.run(command,
                                           shell=True,
                                           check=True,
                                           input=stdin,
                                           stdout=subprocess.PIPE,
                                           stderr=subprocess.PIPE,
                                           timeout=60000)
        usage_after = resource.getrusage(resource.RUSAGE_CHILDREN)
        completed_process.user_time = usage_after.ru_utime - usage_before.ru_utime
        completed_process.sys_time = usage_after.ru_stime - usage_before.ru_stime

        for _, event in events.items():
            event.set()

        return completed_process

    future = job_executor.submit(run_job)
    job_futures.append(future)

def wait_for(resources):
    """Wait for shell process creating the named resource to complete."""
    if isinstance(resources, str):
        resources = [ resources ]
    for res in resources:
        job_resource_events[res].wait()

def save(target, contents=None):
    """Ensure that the given text (default: current contents of this frame)
    exist in the build directory as a file with the given name.  Use an old
    copy from the archive directory if possible."""

    # Use the current frame text by default.
    if contents is None:
        contents = self.text

    # Are we dealing with binary data or text?
    binary = hasattr(contents, 'decode')
    if binary:
        bin_mode_flag = 'b'
    else:
        bin_mode_flag = ''

    # Check for an identically-named, identical-contents file in the archive
    # directory.
    archive_name = os.path.join(archive_directory, target)
    try:
        with open(archive_name, 'r' + bin_mode_flag) as input_file:
            existing_contents = input_file.read()
        exists_already = (existing_contents == contents)
    except IOError:
        exists_already = False

    if exists_already:
        # We have this exact file in the archive directory.  Copy it instead
        # of saving directly.  This keeps the timestamp intact, which can
        # help us elsewhere to tell if things need to be rebuilt.
        try:
            shutil.copy2(archive_name, target)
        except shutil.SameFileError:
            pass

    else:
        # We don't have a file like this anywhere.  Actually save it.
        with open(target, 'w'+bin_mode_flag) as output_file:
            output_file.write(contents)

def recall(target, *sources):
    """If any of the sources are being built by an active shell job, wait
    until that shell job finishes.  Then check whether the given target file
    exists in the archive directory, and if so, if it's newer than all of the
    given sources.  If so, copy it over and return the target.  If not, do
    nothing and return None."""

    archive_target = os.path.join(archive_directory, target)

    ok = True

    for source in sources:
        if source in job_resource_events:
            wait_for(source)

    if not os.path.exists(archive_target):
        print(f"({target}): [does not exist]", end='')
        ok = False
        target_time = None
    else:
        print(f"{target}:", end='')
        target_time = os.path.getmtime(archive_target)
        if sources:
            sources = list(dict.fromkeys(sources))
            for source in sources:
                try:
                    source_time = os.path.getmtime(source)
                except FileNotFoundError as fnfe:
                    raise FileNotFoundError(f'While checking dependencies for '
                                    f'{target}, could not find file {source}. '
                                    f'Full dependency list is {sources}.') from fnfe
                if target_time is not None and source_time > target_time:
                    print(f" ({source})", end='')
                    ok = False
                else:
                    print(f" {source}", end='')
        else:
            print(" [no sources]")

    print()

    if ok:
        if os.path.isfile(archive_target):
            shutil.copy2(archive_target, build_directory)
        else:
            if os.path.exists(target):
                shutil.rmtree(target)
            shutil.copytree(archive_target, os.path.join(build_directory, target))
        return None
    else:
        return target

def at_end():
    """Executed automatically at the end of the run.  Finishes outstanding jobs
    and raises and exceptions that we've been saving up."""
    # Finish up any jobs that are still running.
    exceptions = []
    for future in job_futures:
        try:
            future.result()
        except Exception as exception:  #pylint: disable=broad-except
            exceptions.append(exception)

    # Report any exceptions that occured.
    if len(exceptions) == 1:
        raise exceptions[0]
    if len(exceptions) > 1:
        raise ExceptionGroup("multiple failed jobs", exceptions)

]]]
